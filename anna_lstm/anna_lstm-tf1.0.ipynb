{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 《安娜卡列尼娜》新编——利用TensorFlow构建LSTM模型\n",
    "\n",
    "最近看完了LSTM的一些外文资料，主要参考了Colah的blog以及Andrej Karpathy blog的一些关于RNN的材料，准备动手去实现一个LSTM模型。代码的基础框架来自于Udacity上深度学习纳米学位的课程（付费课程）的一个demo，我刚开始看代码的时候真的是一头雾水，很多东西没有理解，后来反复查阅资料，并我重新对代码进行了学习和修改，对步骤进行了进一步的剖析，下面将一步步用TensorFlow来构建LSTM模型进行文本学习并试图去生成新的文本。\n",
    "\n",
    "关于RNN与LSTM模型本文不做介绍，详情去查阅资料过着去看上面的blog链接，讲的很清楚啦。这篇文章主要是偏向实战，来自己动手构建LSTM模型。\n",
    "\n",
    "数据集来自于外文版《安娜卡列妮娜》书籍的文本文档（本文后面会提供整个project的git链接）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1 数据加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69, 16,  3, 36, 29, 65, 17, 62, 42, 21, 21, 21, 23,  3, 36, 36, 80,\n",
       "       62, 64,  3, 81,  7, 20,  7, 65, 54, 62,  3, 17, 65, 62,  3, 20, 20,\n",
       "       62,  3, 20,  7, 70, 65, 58, 62, 65, 50, 65, 17, 80, 62, 37, 63, 16,\n",
       "        3, 36, 36, 80, 62, 64,  3, 81,  7, 20, 80, 62,  7, 54, 62, 37, 63,\n",
       "       16,  3, 36, 36, 80, 62,  7, 63, 62,  7, 29, 54, 62,  1, 28, 63, 21,\n",
       "       28,  3, 80, 56, 21, 21, 48, 50, 65, 17, 80, 29, 16,  7, 63], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2 分割mini-batch\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"assets/sequence_batching@1x.png\" width=500px>\n",
    "\n",
    "\n",
    "完成了前面的数据预处理操作，接下来就是要划分我们的数据集，在这里我们使用mini-batch来进行模型训练，那么我们要如何划分数据集呢？在进行mini-batch划分之前，我们先来了解几个概念。\n",
    "\n",
    "假如我们目前手里有一个序列1-12，我们接下来以这个序列为例来说明划分mini-batch中的几个概念。首先我们回顾一下，在DNN和CNN中，我们都会将数据分batch输入给神经网络，加入我们有100个样本，如果设置我们的batch_size=10，那么意味着每次我们都会向神经网络输入10个样本进行训练调整参数。同样的，在LSTM中，batch_size意味着每次向网络输入多少个样本，在上图中，当我们设置batch_size=2时，我们会将整个序列划分为6个batch，每个batch中有两个数字。\n",
    "\n",
    "然而由于RNN中存在着“记忆”，也就是循环。事实上一个循环神经网络能够被看做是多个相同神经网络的叠加，在这个系统中，每一个网络都会传递信息给下一个。上面的图中，我们可以看到整个RNN网络由三个相同的神经网络单元叠加起来的序列。那么在这里就有了第二个概念sequence_length（也叫steps），中文叫序列长度。上图中序列长度是3，可以看到将三个字符作为了一个序列。\n",
    "\n",
    "有了上面两个概念，我们来规范一下后面的定义。我们定义一个batch中的序列个数为N（batch_size），定义单个序列长度为M（也就是我们的steps）。那么实际上我们每个batch是一个N x M的数组。在这里我们重新定义batch_size为一个N x M的数组，而不是batch中序列的个数。在上图中，当我们设置N=2， M=3时，我们可以得到每个batch的大小为2 x 3 = 6个字符，整个序列可以被分割成12 / 6 = 2个batch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''\n",
    "    对已有的数组进行mini-batch分割\n",
    "    \n",
    "    arr: 待分割的数组\n",
    "    n_seqs: 一个batch中序列个数\n",
    "    n_steps: 单个序列包含的字符数\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = int(len(arr) / batch_size)\n",
    "    # 这里我们仅保留完整的batch，对于不能整出的部分进行舍弃\n",
    "    arr = arr[:batch_size * n_batches]\n",
    "    \n",
    "    # 重塑\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # inputs\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # targets\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "上面的代码定义了一个generator，调用函数会返回一个generator对象，我们可以获取一个batch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[69 16  3 36 29 65 17 62 42 21]\n",
      " [62  3 81 62 63  1 29 62 40  1]\n",
      " [50  7 63 56 21 21 55 52 65 54]\n",
      " [63 62  9 37 17  7 63 40 62 16]\n",
      " [62  7 29 62  7 54 60 62 54  7]\n",
      " [62  8 29 62 28  3 54 21  1 63]\n",
      " [16 65 63 62 78  1 81 65 62 64]\n",
      " [58 62 79 37 29 62 63  1 28 62]\n",
      " [29 62  7 54 63  4 29 56 62 13]\n",
      " [62 54  3  7  9 62 29  1 62 16]]\n",
      "\n",
      "y\n",
      " [[16  3 36 29 65 17 62 42 21 21]\n",
      " [ 3 81 62 63  1 29 62 40  1  7]\n",
      " [ 7 63 56 21 21 55 52 65 54 60]\n",
      " [62  9 37 17  7 63 40 62 16  7]\n",
      " [ 7 29 62  7 54 60 62 54  7 17]\n",
      " [ 8 29 62 28  3 54 21  1 63 20]\n",
      " [65 63 62 78  1 81 65 62 64  1]\n",
      " [62 79 37 29 62 63  1 28 62 54]\n",
      " [62  7 54 63  4 29 56 62 13 16]\n",
      " [54  3  7  9 62 29  1 62 16 65]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3 模型构建\n",
    "模型构建部分主要包括了输入层，LSTM层，输出层，loss，optimizer等部分的构建，我们将一块一块来进行实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.1 输入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(num_seqs, num_steps):\n",
    "    '''\n",
    "    构建输入层\n",
    "    \n",
    "    num_seqs: 每个batch中的序列个数\n",
    "    num_steps: 每个序列包含的字符数\n",
    "    '''\n",
    "    inputs = tf.placeholder(tf.int32, shape=(num_seqs, num_steps), name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, shape=(num_seqs, num_steps), name='targets')\n",
    "    \n",
    "    # 加入keep_prob\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.2 LSTM层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' \n",
    "    构建lstm层\n",
    "        \n",
    "    keep_prob\n",
    "    lstm_size: lstm隐层中结点数目\n",
    "    num_layers: lstm的隐层数目\n",
    "    batch_size: batch_size\n",
    "\n",
    "    '''\n",
    "    # 构建一个基本lstm单元\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # 添加dropout\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # 堆叠\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.3 输出层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' \n",
    "    构造输出层\n",
    "        \n",
    "    lstm_output: lstm层的输出结果\n",
    "    in_size: lstm输出层重塑后的size\n",
    "    out_size: softmax层的size\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # 将lstm的输出按照列concate，例如[[1,2,3],[7,8,9]],\n",
    "    # tf.concat的结果是[1,2,3,7,8,9]\n",
    "    seq_output = tf.concat(lstm_output, axis=1) # tf.concat(concat_dim, values)\n",
    "    # reshape\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # 将lstm层与softmax层全连接\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # 计算logits\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # softmax层返回概率分布\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.4 训练误差计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    '''\n",
    "    根据logits和targets计算损失\n",
    "    \n",
    "    logits: 全连接层的输出结果（不经过softmax）\n",
    "    targets: targets\n",
    "    lstm_size\n",
    "    num_classes: vocab_size\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot编码\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.5 Optimizer\n",
    "我们知道RNN会遇到梯度爆炸（gradients exploding）和梯度弥散（gradients disappearing)的问题。LSTM解决了梯度弥散的问题，但是gradient仍然可能会爆炸，因此我们采用gradient clippling的方式来防止梯度爆炸。即通过设置一个阈值，当gradients超过这个阈值时，就将它重置为阈值大小，这就保证了梯度不会变得很大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' \n",
    "    构造Optimizer\n",
    "   \n",
    "    loss: 损失\n",
    "    learning_rate: 学习率\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 使用clipping gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.6 模型组合\n",
    "使用tf.nn.dynamic_run来运行RNN序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # 如果sampling是True，则采用SGD\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # 输入层\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # LSTM层\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        # 对输入进行one-hot编码\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # 运行RNN\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # 预测结果\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss 和 optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 参数设置\n",
    "在模型训练之前，我们首先初始化一些参数，我们的参数主要有：\n",
    "\n",
    "- num_seqs: 单个batch中序列的个数\n",
    "- num_steps: 单个序列中字符数目\n",
    "- lstm_size: 隐层结点个数\n",
    "- num_layers: LSTM层个数\n",
    "- learning_rate: 学习率\n",
    "- keep_prob: dropout层中保留结点比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮数: 1/20...  训练步数: 100...  训练误差: 3.0838...  0.3082 sec/batch\n",
      "轮数: 2/20...  训练步数: 200...  训练误差: 2.4530...  0.3091 sec/batch\n",
      "轮数: 2/20...  训练步数: 300...  训练误差: 2.2402...  0.3098 sec/batch\n",
      "轮数: 3/20...  训练步数: 400...  训练误差: 2.0660...  0.3101 sec/batch\n",
      "轮数: 3/20...  训练步数: 500...  训练误差: 1.9340...  0.3112 sec/batch\n",
      "轮数: 4/20...  训练步数: 600...  训练误差: 1.8113...  0.3118 sec/batch\n",
      "轮数: 4/20...  训练步数: 700...  训练误差: 1.7699...  0.3156 sec/batch\n",
      "轮数: 5/20...  训练步数: 800...  训练误差: 1.7050...  0.3192 sec/batch\n",
      "轮数: 5/20...  训练步数: 900...  训练误差: 1.6617...  0.3171 sec/batch\n",
      "轮数: 6/20...  训练步数: 1000...  训练误差: 1.6141...  0.3184 sec/batch\n",
      "轮数: 6/20...  训练步数: 1100...  训练误差: 1.5990...  0.3202 sec/batch\n",
      "轮数: 7/20...  训练步数: 1200...  训练误差: 1.5286...  0.3178 sec/batch\n",
      "轮数: 7/20...  训练步数: 1300...  训练误差: 1.4950...  0.3181 sec/batch\n",
      "轮数: 8/20...  训练步数: 1400...  训练误差: 1.5133...  0.3180 sec/batch\n",
      "轮数: 8/20...  训练步数: 1500...  训练误差: 1.4268...  0.3152 sec/batch\n",
      "轮数: 9/20...  训练步数: 1600...  训练误差: 1.4094...  0.3197 sec/batch\n",
      "轮数: 9/20...  训练步数: 1700...  训练误差: 1.3677...  0.3206 sec/batch\n",
      "轮数: 10/20...  训练步数: 1800...  训练误差: 1.4165...  0.3199 sec/batch\n",
      "轮数: 10/20...  训练步数: 1900...  训练误差: 1.3868...  0.3183 sec/batch\n",
      "轮数: 11/20...  训练步数: 2000...  训练误差: 1.3977...  0.3198 sec/batch\n",
      "轮数: 11/20...  训练步数: 2100...  训练误差: 1.3533...  0.3198 sec/batch\n",
      "轮数: 12/20...  训练步数: 2200...  训练误差: 1.3510...  0.3166 sec/batch\n",
      "轮数: 12/20...  训练步数: 2300...  训练误差: 1.2861...  0.3194 sec/batch\n",
      "轮数: 13/20...  训练步数: 2400...  训练误差: 1.3281...  0.3195 sec/batch\n",
      "轮数: 13/20...  训练步数: 2500...  训练误差: 1.3040...  0.3190 sec/batch\n",
      "轮数: 14/20...  训练步数: 2600...  训练误差: 1.2562...  0.3208 sec/batch\n",
      "轮数: 14/20...  训练步数: 2700...  训练误差: 1.2457...  0.3206 sec/batch\n",
      "轮数: 15/20...  训练步数: 2800...  训练误差: 1.3019...  0.3202 sec/batch\n",
      "轮数: 15/20...  训练步数: 2900...  训练误差: 1.2837...  0.3193 sec/batch\n",
      "轮数: 16/20...  训练步数: 3000...  训练误差: 1.2862...  0.3203 sec/batch\n",
      "轮数: 16/20...  训练步数: 3100...  训练误差: 1.2241...  0.3195 sec/batch\n",
      "轮数: 17/20...  训练步数: 3200...  训练误差: 1.2337...  0.3197 sec/batch\n",
      "轮数: 17/20...  训练步数: 3300...  训练误差: 1.2348...  0.3191 sec/batch\n",
      "轮数: 18/20...  训练步数: 3400...  训练误差: 1.2549...  0.3193 sec/batch\n",
      "轮数: 18/20...  训练步数: 3500...  训练误差: 1.2365...  0.3199 sec/batch\n",
      "轮数: 19/20...  训练步数: 3600...  训练误差: 1.2377...  0.3191 sec/batch\n",
      "轮数: 19/20...  训练步数: 3700...  训练误差: 1.2263...  0.3193 sec/batch\n",
      "轮数: 20/20...  训练步数: 3800...  训练误差: 1.1791...  0.3192 sec/batch\n",
      "轮数: 20/20...  训练步数: 3900...  训练误差: 1.2438...  0.3202 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# 每n轮进行一次变量保存\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            # control the print lines\n",
    "            if counter % 100 == 0:\n",
    "                print('轮数: {}/{}... '.format(e+1, epochs),\n",
    "                      '训练步数: {}... '.format(counter),\n",
    "                      '训练误差: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i3960_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3960_l512.ckpt\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看checkpoints\n",
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5 文本生成\n",
    "现在我们可以基于我们的训练参数进行文本的生成。当我们输入一个字符时，LSTM会预测下一个字符，我们再将新的字符进行输入，这样能不断的循环下去生成本文。\n",
    "\n",
    "为了减少噪音，每次的预测值我会选择最可能的前5个进行随机选择，比如输入h，预测结果概率最大的前五个为[o,e,i,u,b]，我们将随机从这五个中挑选一个作为新的字符，让过程加入随机因素会减少一些噪音的生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    \"\"\"\n",
    "    从预测结果中选取前top_n个最可能的字符\n",
    "    \n",
    "    preds: 预测结果\n",
    "    vocab_size\n",
    "    top_n\n",
    "    \"\"\"\n",
    "    p = np.squeeze(preds)\n",
    "    # 将除了top_n个预测值的位置都置为0\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    # 归一化概率\n",
    "    p = p / np.sum(p)\n",
    "    # 随机选取一个字符\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    \"\"\"\n",
    "    生成新文本\n",
    "    \n",
    "    checkpoint: 某一轮迭代的参数文件\n",
    "    n_sample: 新闻本的字符长度\n",
    "    lstm_size: 隐层结点数\n",
    "    vocab_size\n",
    "    prime: 起始文本\n",
    "    \"\"\"\n",
    "    # 将输入的单词转换为单个字符组成的list\n",
    "    samples = [c for c in prime]\n",
    "    # sampling=True意味着batch的size=1 x 1\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # 加载模型参数，恢复训练\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            # 输入单个字符\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        # 添加字符到samples中\n",
    "        samples.append(int_to_vocab[c])\n",
    "        \n",
    "        # 不断生成字符，直到达到指定数目\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i3960_l512.ckpt'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ther, still he\n",
      "had naturely showed her, and to his white care, and all however\n",
      "had been all about it into him. She did not admit whether the world is\n",
      "she was so such sole as he thrown on the best and death, though he would\n",
      "have anywhere to step in the periods, but the pase had brought him\n",
      "to be said and see in a chance of the sole shame, which he drew up\n",
      "this was such a sense of agriculture, he wished to talk of it. She was\n",
      "not tonare to meet him. He had supposed he would have a sont. But\n",
      "he had both to do this instant would be delighted, and, another was angry,\n",
      "that she did not let the ball of the first sense he had been so doubt that\n",
      "the window he came out of the chations, the chief thirdle sheeps and set\n",
      "that this had to consider the more in the subject, and he had been already\n",
      "fact that there had nothing to get the criving and towards the\n",
      "contrary. They will be so mercialed in his own acquaintances, and he was\n",
      "stopped at her; shreading her hands.\n",
      "\n",
      "\"What, I want to say that I'm sure an official still,\" she said, and the\n",
      "peasant's soul had been ashamed of a sense of the subject, which the\n",
      "coll complaining of a point of her life he had taken the position, and to\n",
      "the dearon to his brother in his soul and so that he was doing that she\n",
      "was sure, he had no sund of the first time he had not seen instead, and\n",
      "she was not the streat of her.\n",
      "\n",
      "\"And that's to say what you might have been true, and what's the court\n",
      "have a love has able to suppose,\" said the old prince.\n",
      "\n",
      "\"I can't let her at her.\"\n",
      "\n",
      "\"What do you say so, I don't know you was so macricialed to her than\n",
      "I'm? He say as to this significances that the mare and one of the\n",
      "misursura to me already, they went on a service in the soul of the\n",
      "position.\"\n",
      "\n",
      "The counting-race was a children he had said.\n",
      "\n",
      "\"I have to be about their childring than you and all so much finden in the\n",
      "word of some man if sure it is interested into the people with a beating\n",
      "too had been saying all these achassions on the card too much the count\n",
      "when they c\n"
     ]
    }
   ],
   "source": [
    "# 选用最终的训练参数作为输入进行文本生成\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"The\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Far disg ter tin soud san and th tha lorenthe soris ther heresd thit\n",
      "os han and tith the sor tis thas thas was ser hes an hes thom hor hasd har sore ta the and whe shared an sin sime thes sisind and sesthe has and ses outile, whan ho what he\n",
      "sase thire with whos and wit ha tin than\n",
      " altet hore tho she andes has and ans, won wistim asthe hes tis he the she sh ates, ant or has the\n",
      " oretin on arin se an oron hes, sitin ser tho shere sile shim ant wom set inle tot he and thet hor his ont ha so ousesin of shir has ho he hes at he\n",
      "sal ale to the the tho wat thase tering tas ter as hin sheresd, af what he thes ale so sha the the aserens, was at hes sasil hot hh the sor sout han ar th shin set aslerang.\n",
      "\"\"\" I sevy and oo le tis\n",
      "\n",
      "or te hor\n",
      "the the shar thes hithe sorilging ons af an hissith at were ang ase an ast hh me he had had set he shan hersin seran tothend he thess af terilse\n",
      "afd an oos whas an thes and hit thers tis tas an hos whasd\n",
      "herinn sat in he the was anter sill as the sithise, wat tha\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farrey what the postors was the\n",
      "conterst time\n",
      "a dood and won't strend him heard in a seamed, and were\n",
      "the mother's and the ploosed of tho cound than any seen a mile, the prinecs in the mand and\n",
      "there arrong the hand tooking at\n",
      "the mose trise, but as a munce said to the say, and and all how that he was\n",
      "to be same in time to asterst in tell to ter that a concemation of white the condirence of the point and at the conversoat this in has basider that, the complece, was\n",
      "staring at out thing what he should sat her hands the parry on the carriage at her, was insented in a lattres than her, had began asked\n",
      "at her to shall on his simpis of the compontences, but to see howe the proves of her his\n",
      "and shroked at that the sender to his been, and the might a sent was a fince seat of them, and a can all would her feeling and had been, so saying to his side only the charcion of the manter, say and\n",
      "telling with her head to a preas with\n",
      "the pearant, but at in theig to so to be into the such the carned and h\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i1000_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farrenance himself all her fasher of her heart and stayed in\n",
      "a long at the card, and so andey a serial conversation to him. He came\n",
      "to think, and the course taken, and the children thought, and and white\n",
      "consider and straight to thit, and was all of shilly before to set it,\n",
      "and had askered him on his house, and he had tell him.\n",
      "\n",
      "\"The pressine in the subject that has been tried to be doing that that\n",
      "has been a particular and same in their more, and he had said that\n",
      "had alleass on the different conversation and son thing in the same\n",
      "aspice. In hos before they.\"\n",
      "\n",
      "\"Women well. And I suppressed the door and a man. There've not to\n",
      "go that hurried in him and this, and\n",
      "anying.\" said Veslovsky, growing an hour off over\n",
      "silent, but the daughter has stood as a group and stanging at hham in\n",
      "the conversation of the confections. And sooused to the stop of\n",
      "his sensitions without sense of so to told him, and there was in it out to his\n",
      "from his face, and the prase was in an earing that still whether she ha\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i2000_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
